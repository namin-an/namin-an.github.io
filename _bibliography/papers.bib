---
---
@article{an2025can,
  title={Can LVLMs and Automatic Metrics Capture Underlying Preferences of Blind and Low-Vision Individuals for Navigational Aid?},
  author={An, Na Min and Kim, Eunki and Kang, Wan Ju and Kim, Sangryul and Shim, Hyunjung and Thorne, James},
  journal={arXiv preprint arXiv:2502.14883},
  year={2025},
  preview={eye4b.png},
  url=https://arxiv.org/abs/2502.14883,
  arxiv="2502.14883",
  pdf={https://arxiv.org/pdf/2502.14883},
  abstract="Vision is a primary means of how humans perceive the environment, but Blind and Low-Vision (BLV) people need assistance understanding their surroundings, especially in unfamiliar environments. The emergence of semantic-based systems as assistance tools for BLV users has motivated many researchers to explore responses from Large Vision-Language Models (LVLMs). However, it has yet been studied preferences of BLV users on diverse types/styles of responses from LVLMs, specifically for navigational aid. To fill this gap, we first construct Eye4B dataset, consisting of human-validated 1.1k curated outdoor/indoor scenes with 5-10 relevant requests per scene. Then, we conduct an in-depth user study with eight BLV users to evaluate their preferences on six LVLMs from five perspectives: Afraidness, Nonactionability, Sufficiency, and Conciseness. Finally, we introduce Eye4B benchmark for evaluating alignment between widely used model-based image-text metrics and our collected BLV preferences. Our work can be set as a guideline for developing BLV-aware LVLMs towards a Barrier-Free AI system."
}

@article{waheed2025image,
  title={Image Embedding Sampling Method for Diverse Captioning},
  author={Waheed, Sania and An, Na Min},
  journal={arXiv preprint arXiv:2502.10118},
  year={2025},
  preview={hbop.png},
  url=https://arxiv.org/abs/2502.10118,
  pdf={https://arxiv.org/abs/2502.10118},
  arxiv="2502.10118",
  abstract="Image Captioning for state-of-the-art VLMs has significantly improved over time; however, this comes at the cost of increased computational complexity, making them less accessible for resource-constrained applications such as mobile devices and assistive technologies. Alternatively, smaller VLMs prioritize high-level scene descriptions, overlooking finer details that contribute to a richer understanding of an image. In this paper, we introduce a training-free framework that enhances caption diversity and informativeness by explicitly attending to distinct image regions using a comparably small VLM, BLIP, as the backbone. Our approach leverages structured segmentation to produce hierarchical representations that capture both global and localized semantics. Without requiring additional model training, we demonstrate that our method allows smaller VLMs to achieve performance comparable to larger models in terms of image-caption alignment, semantic integrity, and diversity. We evaluate our framework on MSCOCO, Flickr30k, and Nocaps test datasets, achieving a Div-2 score of 0.735, 0.750, and 0.748 for each dataset respectively, while maintaining strong image-caption relevancy and semantic integrity with the human-annotated captions."
}

@article{bayramli2025diffusion,
  title={Diffusion Models Through a Global Lens: Are They Culturally Inclusive?},
  author={Bayramli, Zahra and Suleymanzade, Ayhan and An, Na Min and Ahmad, Huzama and Kim, Eunsu and Park, Junyeong and Thorne, James and Oh, Alice},
  journal={arXiv preprint arXiv:2502.08914},
  year={2025},
  url=https://arxiv.org/abs/2502.08914,
  pdf={https://arxiv.org/abs/2502.08914},
  preview={framework.png},
  arxiv="2502.08914",
  abstract="Text-to-image diffusion models have recently enabled the creation of visually compelling, detailed images from textual prompts. However, their ability to accurately represent various cultural nuances remains an open question. In our work, we introduce CultDiff benchmark, evaluating state-of-the-art diffusion models whether they can generate culturally specific images spanning ten countries. We show that these models often fail to generate cultural artifacts in architecture, clothing, and food, especially for underrepresented country regions, by conducting a fine-grained analysis of different similarity aspects, revealing significant disparities in cultural relevance, description fidelity, and realism compared to real-world reference images. With the collected human evaluations, we develop a neural-based image-image similarity metric, namely, CultDiff-S, to predict human judgment on real and generated images with cultural artifacts. Our work highlights the need for more inclusive generative AI systems and equitable dataset representation over a wide range of cultures."
}

@article{an2025machine,
  title={Machine Learning Techniques for Simulating Human Psychophysical Testing of Low-Resolution Phosphene Face Images in Artificial Vision},
  author={An, Na Min and Roh, Hyeonhee and Kim, Sein and Kim, Jae Hun and Im, Maesoon},
  journal={Advanced Science},
  pages={2405789},
  year={2025},
  publisher={Wiley Online Library},
  preview={adv_sci.png},
  url=https://advanced.onlinelibrary.wiley.com/doi/10.1002/advs.202405789,
  pdf={https://advanced.onlinelibrary.wiley.com/doi/epdf/10.1002/advs.202405789},
  selected={true},
  abstract="To evaluate the quality of artificial visual percepts generated by emerging methodologies, researchers often rely on labor-intensive and tedious human psychophysical experiments. These experiments necessitate repeated iterations upon any major/minor modifications in the hardware/software configurations. Here, the capacity of standard machine learning (ML) models is investigated to accurately replicate quaternary match-to-sample tasks using low-resolution facial images represented by arrays of phosphenes as input stimuli. Initially, the performance of the ML models trained to approximate innate human facial recognition abilities across a dataset comprising 3600 phosphene images of human faces is analyzed. Subsequently, due to the time constraints and the potential for subject fatigue, the psychophysical test is limited to presenting only 720 low-resolution phosphene images to 36 human subjects. Notably, the superior model adeptly mirrors the behavioral trend of human subjects, offering precise predictions for 8 out of 9 phosphene quality levels on the overlapping test queries. Subsequently, human recognition performances for untested phosphene images are predicted, streamlining the process and minimizing the need for additional psychophysical tests. The findings underscore the transformative potential of ML in reshaping the research paradigm of visual prosthetics, facilitating the expedited advancement of prostheses."
}

@article{an2024i0t,
  title={I0T: Embedding Standardization Method Towards Zero Modality Gap},
  author={An, Na Min and Kim, Eunki and Thorne, James and Shim, Hyunjung},
  journal={arXiv preprint arXiv:2412.14384},
  year={2024},
  preview={I0T.png},
  url=https://arxiv.org/pdf/2412.14384,
  arxiv="2412.14384",
  abstract="Contrastive Language-Image Pretraining (CLIP) enables zero-shot inference in downstream tasks such as image-text retrieval and classification. However, recent works extending CLIP suffer from the issue of modality gap, which arises when the image and text embeddings are projected to disparate manifolds, deviating from the intended objective of image-text contrastive learning. We discover that this phenomenon is linked to the modality-specific characteristic that each image/text encoder independently possesses and propose two methods to address the modality gap: (1) a post-hoc embedding standardization method, I0Tpost that reduces the modality gap approximately to zero and (2) a trainable method, I0Tasync, to alleviate the modality gap problem by adding two normalization layers for each encoder. Our I0T framework can significantly reduce the modality gap while preserving the original embedding representations of trained models with their locked parameters. In practice, I0Tpost can serve as an alternative explainable automatic evaluation metric of widely used CLIPScore (CLIP-S)."
}

@misc{2409.07787,
  author = {Chung, Woojin and Hong, Jiwoo and An, Na Min and Thorne, James and Yun, Se-Young},
  title = {Stable Language Model Pre-training by Reducing Embedding Variability},
  journal={Proceedings of the 2024 Conference on Empirical Methods in Natural Language Processing (EMNLP)},
  year = {2024},
  preview={tev_grad_var.png},
  url=https://arxiv.org/abs/2409.07787,
  arxiv = "2409.07787",
  abstract="Stable pre-training is essential for achieving better-performing language models. However, tracking pre-training stability by calculating gradient variance at every step is impractical due to the significant computational costs. We explore Token Embedding Variability (TEV) as a simple and efficient proxy for assessing pre-training stability in language models with pre-layer normalization, given that shallower layers are more prone to gradient explosion (section 2.2). Moreover, we propose Multi-head Low-Rank Attention (MLRA) as an architecture to alleviate such instability by limiting the exponential growth of output embedding variance, thereby preventing the gradient explosion (section 3.2). Empirical results on GPT-2 with MLRA demonstrate increased stability and lower perplexity, particularly in deeper models."
}

@inproceedings{an-etal-2024-capturing,
    title = "Capturing the Relationship Between Sentence Triplets for {LLM} and Human-Generated Texts to Enhance Sentence Embeddings",
    author = "An, Na Min  and
      Waheed, Sania  and
      Thorne, James",
    editor = "Graham, Yvette  and
      Purver, Matthew",
    booktitle = "Findings of the Association for Computational Linguistics: EACL 2024",
    month = mar,
    year = "2024",
    address = "St. Julian{'}s, Malta",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2024.findings-eacl.43",
    pages = "624--638",
    preview={pna.png},
    pdf={https://aclanthology.org/2024.findings-eacl.43.pdf},
    abstract = "Deriving meaningful sentence embeddings is crucial in capturing the semantic relationship between texts. Recent advances in building sentence embedding models have centered on replacing traditional human-generated text datasets with those generated by LLMs. However, the properties of these widely used LLM-generated texts remain largely unexplored. Here, we evaluate the quality of the LLM-generated texts from four perspectives (Positive Text Repetition, Length Difference Penalty, Positive Score Compactness, and Negative Text Implausibility) and find that there exists an inherent difference between human and LLM-generated datasets. To further enhance sentence embeddings using both human and LLM-generated datasets, we propose a novel loss function that incorporates Positive-Negative sample Augmentation (PNA) within the contrastive learning objective. Our results demonstrate that PNA effectively mitigates the sentence anisotropy problem in Wikipedia corpus (-7{\%} compared to CLHAIF) and simultaneously improves the Spearman{'}s correlation in standard Semantic Textual Similarity (STS) tasks (+1.47{\%} compared to CLHAIF).",
}

@inproceedings{lee-etal-2023-large,
    title = "Can Large Language Models Capture Dissenting Human Voices?",
    author = "Lee$*$, Noah  and
      An$*$, Na Min  and
      Thorne, James",
    editor = "Bouamor, Houda  and
      Pino, Juan  and
      Bali, Kalika",
    booktitle = "Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing (EMNLP)",
    month = dec,
    year = "2023",
    address = "Singapore",
    publisher = "Association for Computational Linguistics",
    url = "2305.13788",
    doi = "10.18653/v1/2023.emnlp-main.278",
    pages = "4569--4585",
    arxiv = "2305.13788",
    preview={hum_vs_llm.png},
    pdf={https://aclanthology.org/2023.emnlp-main.278.pdf},
    abstract = "Large language models (LLMs) have shown impressive achievements in solving a broad range of tasks. Augmented by instruction fine-tuning, LLMs have also been shown to generalize in zero-shot settings as well. However, whether LLMs closely align with the human disagreement distribution has not been well-studied, especially within the scope of natural language inference (NLI). In this paper, we evaluate the performance and alignment of LLM distribution with humans using two different techniques to estimate the multinomial distribution: Monte Carlo Estimation (MCE) and Log Probability Estimation (LPE). As a result, we show LLMs exhibit limited ability in solving NLI tasks and simultaneously fail to capture human disagreement distribution. The inference and human alignment performances plunge even further on data samples with high human disagreement levels, raising concerns about their natural language understanding (NLU) ability and their representativeness to a larger human population.",
    note = {$*$ indicates equal contribution.}}
}

@INPROCEEDINGS{10191870,
  author={An, Na Min and Roh, Hyeonhee and Kim, Sein and Kim, Jae Hun and Im, Maesoon},
  booktitle={2023 International Joint Conference on Neural Networks (IJCNN)}, 
  title={Reinforcement Learning Framework to Simulate Short-Term Learning Effects of Human Psychophysical Experiments Assessing the Quality of Artificial Vision}, 
  year={2023},
  volume={},
  number={},
  pages={1-8},
  doi={10.1109/IJCNN54540.2023.10191870},
  preview={reinforcement_learning.jpg},
  pdf={https://ieeexplore.ieee.org/abstract/document/10191870},
  abstract = "The quality of the artificial vision produced by visual prostheses has traditionally been evaluated by human psychophysical tests using images expressed with an array of phosphenes. However, such experiments involving human subjects are considerably time-consuming and labor-intensive. One potential solution may be implementing an efficient approach to assist or replace psychophysical experiments showing a short-term learning effect in human subjects. The present work developed a reinforcement learning (RL)-based feedback framework which built artificial agents to emulate the behavioral changes in the learning process of human subjects over the experimental trials. In our framework, we first trained an agent which can gradually learn to identify 720 faces presented in low-resolution phosphene images with feedback rewards received from the agreement in the perception of nine training human subjects. Then, in the automating stage of the framework, we created nine RL agents. By testing those agents, we found the RL agents mimicked the learning effects of nine test human subjects better than nine instances of a supervised learning (SL) model. Given the similar outcomes with human tests and the time efficiency of RL, our framework may expedite the development of visual prosthetic systems by at least partially replacing laborious human psychophysical experiments."
}

@misc{maesoon2023artificial,
  title={Artificial Vision Parameter Learning and Automating Method for Improving Visual Prosthetic Systems},
  author={Im, Maesoon and Roh, Hyeonhee and An, Na Min and Kim, Jae Hun},
  year={2023},
  month=jun # "~8",
  publisher={Google Patents},
  preview={patent.png},
  pdf={https://patents.google.com/patent/US20230177396A1/en},
  note={US Patent App. 18/075,555}
}

@article{an2021machine,
  title={Machine Learning Approaches as An Alternative to Human Psychophysical Tests of Prosthetic Vision (abstract)},
  author={An, Na Min and Roh, Hyeonhee and Jung, Soomin and Kim, Eun Ju and Im, Maesoon},
  journal={2021 43rd Annual International Conference of the IEEE Engineering in Medicine & Biology Society (EMBC)},
  year={2021},
  preview={pca_vs_cnn.png},
  pdf={https://paperhost.org/proceedings/embs/EMBC21/files/2302.pdf},
  abstract="Microelectronic retinal prostheses can evoke artificial visual percepts to blind individuals. Among several factors, expression of original images using limited resources (i.e., small number of pixels and low gray scales) is critical for improved recognition of artificial vision. To assess effectiveness of new image processing techniques, prosthetic researchers have performed psychophysical tests with normally-sighted subjects. Here, we evaluated facial recognition task performances of two machine learning (ML) models for phosphene images in various conditions, representing several electrical stimulation cases. Both ML models showed correct response ratios with trends expected from psychophysical studies with human subjects. Our results suggest ML approaches would be useful for evaluating newly-developed artificial vision image processing techniques. In particular, the use of ML approaches is expected to substantially reduce time and cost needed for psychophysical testing of phosphene images."
}